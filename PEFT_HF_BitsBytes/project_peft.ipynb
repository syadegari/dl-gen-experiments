{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269c5999",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77e4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdc840",
   "metadata": {},
   "source": [
    "# Load and Tokenize IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76c5226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = ['train', 'test']\n",
    "subset_size = 10_000\n",
    "\n",
    "dataset = {\n",
    "    split: load_dataset('imdb', split=split).shuffle(seed=101).select(range(subset_size))\n",
    "    for split in splits\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = {\n",
    "    name: dset.map(\n",
    "        lambda x: tokenizer(x['text'], padding=True, truncation=True),\n",
    "        batched=True\n",
    "    ) for name, dset in dataset.items()\n",
    "}\n",
    "\n",
    "for dset in tokenized_dataset.values():\n",
    "    dset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "dataloader = {name: DataLoader(dset, batch_size=64) for name, dset in tokenized_dataset.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=2,\n",
    "    id2label={0: 'Negative', 1: 'Positive'}, \n",
    "    label2id={'Negative': 0, 'Positive': 1},\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")        \n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e8d5b",
   "metadata": {},
   "source": [
    "# Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8878532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(predictions, labels):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def evaluate_batch(model, tokenized_inputs, labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b77138",
   "metadata": {},
   "source": [
    "# Evaluate Pretrained GPT-2 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d46a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [02:24<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4994,\n",
       " 'f1': array([0.66577647, 0.00318598]),\n",
       " 'precision': array([0.49969934, 0.36363636]),\n",
       " 'recall': array([0.9972, 0.0016])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels, batch_predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader['test']):\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        predictions, labels = evaluate_batch(model, {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}, batch['label'])\n",
    "        batch_labels.append(labels)        \n",
    "        batch_predictions.append(predictions)\n",
    "\n",
    "get_metrics(np.concatenate(batch_predictions), np.concatenate(batch_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1654be",
   "metadata": {},
   "source": [
    "The accuracy is almost 50%, and since the dataset is balanced, the model's predictions are random. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fe032",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "The followings reuse some of the code from before and wraps them in functions in order to facilitate the experiments setups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e710e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_parameters(model):\n",
    "    param_dict = defaultdict(int)\n",
    "    param_sum = 0\n",
    "    trainable_sum = 0\n",
    "    for param in model.parameters():\n",
    "        param_sum += param.numel()\n",
    "        param_dict[str(param.dtype)] += param.numel()\n",
    "\n",
    "        if param.requires_grad:\n",
    "            trainable_sum += param.numel()\n",
    "    return {\n",
    "        'total parameters': f'{param_sum / 1e6:.2f} Million',\n",
    "        'trainable parameters': trainable_sum,\n",
    "        'parameter count per data type': dict(param_dict)\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ce929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "path_notebook = \"/home/srn/Documents/code/gen_ai_course/p1_PEFT_GPT2/\"\n",
    "\n",
    "\n",
    "def get_tokenizer() -> AutoTokenizer:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_tokenized_datasets(train_samples: int, validation_samples: int, test_samples: int, seed: int) -> dict:\n",
    "    splits = [\"train\", \"test\"]\n",
    "    dataset = {split: load_dataset(\"imdb\", split=split) for split in splits}\n",
    "\n",
    "    dataset[\"train\"] = dataset[\"train\"].shuffle(seed).select(range(train_samples))\n",
    "    validation_test_dsets = dataset[\"test\"].train_test_split(\n",
    "        train_size=validation_samples, test_size=test_samples, shuffle=True, seed=seed\n",
    "    )\n",
    "    dataset[\"validation\"] = validation_test_dsets[\"train\"]\n",
    "    dataset[\"test\"] = validation_test_dsets[\"test\"]\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    tokenized_dataset = {\n",
    "        name: dset.map(lambda x: tokenizer(x[\"text\"], padding=True, truncation=True), batched=True)\n",
    "        for name, dset in dataset.items()\n",
    "    }\n",
    "\n",
    "    for dset in tokenized_dataset.values():\n",
    "        dset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "def compute_metrics(eval_prediction):\n",
    "    predictions, labels = eval_prediction\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "\n",
    "def freeze_model(model) -> None:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def prepare_model(tokenizer, bits_and_bytes_config=None, lora_config=None, use_gradient_checkpointing=False):\n",
    "    if bits_and_bytes_config:\n",
    "        assert lora_config is not None\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        num_labels=2,\n",
    "        id2label={0: \"Negative\", 1: \"Positive\"},\n",
    "        label2id={\"Negative\": 0, \"Positive\": 1},\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        quantization_config=bits_and_bytes_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "    if bits_and_bytes_config:\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)\n",
    "\n",
    "    if lora_config:\n",
    "        # lora config freezes the model parameters of the base\n",
    "        return get_peft_model(model, lora_config)\n",
    "    else:\n",
    "        # need to manually freeze the base mode\n",
    "        freeze_model(model.base_model)\n",
    "        return model\n",
    "\n",
    "\n",
    "def get_model_info(model):\n",
    "    num_trainable_tensors = 0\n",
    "    num_trainable_parameters = 0\n",
    "    trainable_bytes = 0\n",
    "\n",
    "    dtype_bytes = {torch.float32: 4, torch.float16: 2, torch.bfloat16: 2, torch.uint8: 1, torch.int8: 1}\n",
    "    dtypes_trainable = []\n",
    "    dtypes_non_trainable = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            num_trainable_tensors += 1\n",
    "            num_trainable_parameters += param.numel()\n",
    "            trainable_bytes += param.numel() * dtype_bytes[param.dtype]\n",
    "            dtypes_trainable.append(param.dtype)\n",
    "        else:\n",
    "            dtypes_non_trainable.append(param.dtype)\n",
    "    return {\n",
    "        \"trainable parameters (1e6)\": num_trainable_parameters / 1e6,\n",
    "        \"trainable parameters size (MB)\": trainable_bytes / (1024 * 1024),\n",
    "        \"trainable tensors\": num_trainable_tensors,\n",
    "        \"memory footprint (MB)\": model.get_memory_footprint() / (1024 * 1024),\n",
    "        \"dtypes trainable\": set(dtypes_trainable),\n",
    "        \"dtypes non-trainable\": set(dtypes_non_trainable),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_trainer(\n",
    "    model, tokenized_dataset, tokenizer, n_epochs, max_steps, batch_size, lr, weight_decay, eval_steps, tensorboard_name\n",
    ") -> Trainer:\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path_notebook + \"/evals/baseline\" + tensorboard_name,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=n_epochs,\n",
    "        max_steps=max_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf3452d",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer, Tokenized Datasets and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a3516",
   "metadata": {},
   "source": [
    "Use a subset of dataset for training as before. Specifically:\n",
    "- `10000` samples for training\n",
    "- `1000` samples for validation and testing each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89136eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 10000\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1000\n",
      "}), 'validation': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1000\n",
      "})}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdaac0108a84a4b821899974ca81cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tokenized_dataset = get_tokenized_datasets(train_samples=10000, validation_samples=1000, test_samples=1000, seed=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987b898",
   "metadata": {},
   "source": [
    "We use the following three models for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478dbd4",
   "metadata": {},
   "source": [
    "### Base Model with the classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c2720006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_f32 = prepare_model(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e322b",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "59ebd185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_f32_lora = prepare_model(tokenizer, lora_config=LORA_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143df39c",
   "metadata": {},
   "source": [
    "### LoRA with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f16a37b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_nf4_lora_with_chkpoint = prepare_model(\n",
    "    tokenizer, bits_and_bytes_config=BNB_CONFIG, lora_config=LORA_CONFIG, use_gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd09f89",
   "metadata": {},
   "source": [
    "# Some Notes and Explanation of LoRA and Quantized Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8ff3f",
   "metadata": {},
   "source": [
    "In the above, we prepared 3 different models for training:\n",
    "\n",
    "- `model_f32` is the vanilla GPT2 with a classifier head. It contains only a handful of trainable parameters since we only have two categories and the output is `768` dimensional (about `1.5 K` parameters)\n",
    "- `model_f32_lora` is the LoRA adjust models. This models has about `600_000` trainable parameters since we use a low rank approximation of dimension `4` \n",
    "\n",
    "```python\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "```\n",
    "\n",
    "which is then attached to all the frozen modules in both attention and mlp layers in all the block (GPT2 has 12 block in total). \n",
    "\n",
    "One can inspect one of these blocks more closely\n",
    "\n",
    "```python\n",
    "\n",
    ">>> block_lora_f32 = model_f32_lora.base_model.model.transformer.h[0].attn.c_attn\n",
    ">>> block_lora_f32\n",
    "\n",
    "lora.Linear(\n",
    "  (base_layer): Conv1D(nf=2304, nx=768)\n",
    "  (lora_dropout): ModuleDict(\n",
    "    (default): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (lora_A): ModuleDict(\n",
    "    (default): Linear(in_features=768, out_features=4, bias=False)\n",
    "  )\n",
    "  (lora_B): ModuleDict(\n",
    "    (default): Linear(in_features=4, out_features=2304, bias=False)\n",
    "  )\n",
    "  (lora_embedding_A): ParameterDict()\n",
    "  (lora_embedding_B): ParameterDict()\n",
    "  (lora_magnitude_vector): ModuleDict()\n",
    ")\n",
    "```\n",
    "\n",
    "with `lora_A` and `lora_B` being the low rank approximation, in this case, of 4. These `lora_A` and `lora_B` matrices are the trainable parameters of the model.\n",
    "\n",
    "- `model_nf4_with_chkpoint` uses quantization as well as LoRA, plus recomputing the intermediate activations to save memory (called checkpointing), thus enabling using larger batch sizes and/or larger models. This stores certain layer weights in 4-bit and it appears that the number of parameters are reduces, but the reduction stems from the fact that under the hood, weights are stored in 8-bit variables (so two 4-bit parameters can be packed inside a single 8-bit bit parameter):\n",
    "\n",
    "```python\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "```\n",
    " Similar to LoRA model, one can peek into individual attention heads to see how the quantization is performed (not all the details but at least gain some insight into the workings of the bitsandbytes module):\n",
    "```python\n",
    ">>> block_nf4 = model_nf4_lora_with_chkpoint.base_model.model.transformer.h[0].attn.c_attn\n",
    ">>> block_nf4\n",
    "\n",
    "lora.Linear4bit(\n",
    "  (base_layer): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
    "  (lora_dropout): ModuleDict(\n",
    "    (default): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (lora_A): ModuleDict(\n",
    "    (default): Linear(in_features=768, out_features=4, bias=False)\n",
    "  )\n",
    "  (lora_B): ModuleDict(\n",
    "    (default): Linear(in_features=4, out_features=2304, bias=False)\n",
    "  )\n",
    "  (lora_embedding_A): ParameterDict()\n",
    "  (lora_embedding_B): ParameterDict()\n",
    "  (lora_magnitude_vector): ModuleDict()\n",
    ")\n",
    "```\n",
    "\n",
    "The LoRA, trainable parameters, remain 32-bit:\n",
    "\n",
    "```python\n",
    ">>> for param in block_nf4.lora_A.parameters():\n",
    "...    print(param.shape, param.dtype)\n",
    "\n",
    "torch.Size([4, 768]) torch.float32\n",
    "```\n",
    "\n",
    "whereas the non-trainable parameters have been quantized to 4bit values:\n",
    "\n",
    "```python\n",
    ">>> for param in block_nf4.base_layer.parameters():\n",
    "...    print(param.shape, param.dtype)\n",
    "\n",
    "torch.Size([884736, 1]) torch.uint8\n",
    "torch.Size([2304]) torch.float32\n",
    "```\n",
    "\n",
    "interestingly, the bias term remains 32-bit where the matrix weights have been quantized and packed into a 1D tensor. I am not sure why this has been done like this (in terms of the 1D storation of the tensor). The number of parameters and the dtype are consistent with 4-bit quantization, since we originally had `[768, 2304]` float-32 parameters. That's `1769472` parameters, and in 4-bit, we can store 2 of such parameters inside a byte (uint8), thus halving the number of parameters in each quantized tensor, matching the `[884736, 1]` shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3615226",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc479a9",
   "metadata": {},
   "source": [
    "### Training Base Model (Only the Classifier Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5ea3703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4256/3451124879.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 05:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583060</td>\n",
       "      <td>0.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460398</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484903</td>\n",
       "      <td>0.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.418062</td>\n",
       "      <td>0.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.422771</td>\n",
       "      <td>0.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.411517</td>\n",
       "      <td>0.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.397840</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.401065</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3832074701786041,\n",
       " 'eval_accuracy': 0.836,\n",
       " 'eval_runtime': 18.4467,\n",
       " 'eval_samples_per_second': 54.21,\n",
       " 'eval_steps_per_second': 2.168,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_f32 = get_trainer(\n",
    "    model_f32,\n",
    "    tokenized_dataset,\n",
    "    tokenizer,\n",
    "    n_epochs=100,\n",
    "    batch_size=25,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-2,\n",
    "    eval_steps=50,\n",
    "    max_steps=400,\n",
    "    tensorboard_name=\"f32\",\n",
    ")\n",
    "trainer_f32.train()\n",
    "trainer_f32.evaluate(eval_dataset=tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74563b1f",
   "metadata": {},
   "source": [
    "### Training LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a19a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4256/3451124879.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 17:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.740409</td>\n",
       "      <td>0.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690592</td>\n",
       "      <td>0.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562058</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.329285</td>\n",
       "      <td>0.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.290022</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.458923</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.267724</td>\n",
       "      <td>0.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.898000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.291053</td>\n",
       "      <td>0.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.307727</td>\n",
       "      <td>0.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.309161</td>\n",
       "      <td>0.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.273994</td>\n",
       "      <td>0.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.270786</td>\n",
       "      <td>0.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.258798</td>\n",
       "      <td>0.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.270573</td>\n",
       "      <td>0.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.276177</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.284317</td>\n",
       "      <td>0.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.274647</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.264977</td>\n",
       "      <td>0.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.275960</td>\n",
       "      <td>0.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.271793</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.274977</td>\n",
       "      <td>0.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.275407</td>\n",
       "      <td>0.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.277783</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2912628650665283,\n",
       " 'eval_accuracy': 0.903,\n",
       " 'eval_runtime': 21.0558,\n",
       " 'eval_samples_per_second': 47.493,\n",
       " 'eval_steps_per_second': 5.937,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_f32_lora = get_trainer(\n",
    "    model_f32_lora,\n",
    "    tokenized_dataset,\n",
    "    tokenizer,\n",
    "    n_epochs=100,\n",
    "    batch_size=8,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2,\n",
    "    eval_steps=50,\n",
    "    max_steps=1250,\n",
    "    tensorboard_name=\"f32_lora\",\n",
    ")\n",
    "trainer_f32_lora.train()\n",
    "trainer_f32_lora.evaluate(eval_dataset=tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829d57f",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc8f9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f32_lora.save_pretrained('saved_f32_lora')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f731c",
   "metadata": {},
   "source": [
    "Load back the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "69f923fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4256/3451124879.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.29126283526420593,\n",
       " 'eval_model_preparation_time': 0.0031,\n",
       " 'eval_accuracy': 0.903,\n",
       " 'eval_runtime': 20.5994,\n",
       " 'eval_samples_per_second': 48.545,\n",
       " 'eval_steps_per_second': 3.058}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained('saved_f32_lora')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(peft_config.base_model_name_or_path, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, 'saved_f32_lora')\n",
    "\n",
    "trainer = get_trainer(\n",
    "    model,\n",
    "    tokenized_dataset,\n",
    "    tokenizer,\n",
    "    n_epochs=1,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2,\n",
    "    eval_steps=50,\n",
    "    max_steps=-1,\n",
    "    tensorboard_name=\"f32_lora\",\n",
    ")\n",
    "\n",
    "trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2d824",
   "metadata": {},
   "source": [
    "### Training Quantized-LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "47517285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4256/3451124879.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/srn/Documents/code/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 18:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.736271</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.631770</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.414766</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.294711</td>\n",
       "      <td>0.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.289729</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288955</td>\n",
       "      <td>0.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.280963</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270410</td>\n",
       "      <td>0.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.260655</td>\n",
       "      <td>0.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.254403</td>\n",
       "      <td>0.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.255635</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.256925</td>\n",
       "      <td>0.907000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srn/Documents/code/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24462735652923584,\n",
       " 'eval_accuracy': 0.905,\n",
       " 'eval_runtime': 24.7907,\n",
       " 'eval_samples_per_second': 40.338,\n",
       " 'eval_steps_per_second': 2.541,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_nf4_lora_with_chkpoint = get_trainer(\n",
    "    model_nf4_lora_with_chkpoint,\n",
    "    tokenized_dataset,\n",
    "    tokenizer,\n",
    "    n_epochs=1,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2,\n",
    "    eval_steps=50,\n",
    "    max_steps=-1,\n",
    "    tensorboard_name=\"nf4_lora_with_chkpoint\",\n",
    ")\n",
    "trainer_nf4_lora_with_chkpoint.train()\n",
    "trainer_nf4_lora_with_chkpoint.evaluate(eval_dataset=tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016bf7c",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b6c1f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nf4_lora_with_chkpoint.save_pretrained('saved_nf4_lora')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9db7ad",
   "metadata": {},
   "source": [
    "Load back the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a0730aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4256/3451124879.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.300048828125,\n",
       " 'eval_model_preparation_time': 0.0008,\n",
       " 'eval_accuracy': 0.878,\n",
       " 'eval_runtime': 19.8745,\n",
       " 'eval_samples_per_second': 50.316,\n",
       " 'eval_steps_per_second': 3.17}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained('saved_nf4_lora')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(peft_config.base_model_name_or_path, load_in_4bit=True, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, 'saved_nf4_lora')\n",
    "\n",
    "trainer = get_trainer(\n",
    "    model,\n",
    "    tokenized_dataset,\n",
    "    tokenizer,\n",
    "    n_epochs=1,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2,\n",
    "    eval_steps=50,\n",
    "    max_steps=-1,\n",
    "    tensorboard_name=\"nf4_lora_with_chkpoint\",\n",
    ")\n",
    "\n",
    "trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e3782",
   "metadata": {},
   "source": [
    "# Conclusions/Suggestions\n",
    "\n",
    "- Both LoRA and Quantized models achieved higher accuracy than the base model on the test set, 90% and 88% respectively versus the accuracy of 83% for the base model. \n",
    "- Quantized model with checkpointing, `model_nf4_lora_with_chkpoint`, can handle bigger batch sizes or a bigger model. It is better to use an even larger model in this case and compare it to GPT2-LoRA. \n",
    "- Comparing the three presented variants is non-trivial. While all three are trained on the same number of epochs, a better and more involved approach would be to match them by compute budget (GPU hours and memory)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
