{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](./img/workflow1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a demonstration of Retrieval-Augmented Generation using (RAG) OpenAI API (`gpt-3.5`). At its core, RAG uses additional context to the original query that is selected from an external (vectorized) dataset, in order to enahnce the query response, or provide for missing information that were not part of the training dataset. It is therefore an alternative to fine tuning. The above diagrams contrast the traditional workflow, where the user inputs are fed into the LLM and outputs are generated, with RAG.\n",
    "\n",
    "There are several steps to implementing RAG that are listed below:\n",
    "- __Find out where the LLM is lacking performance__: Based on defined requirements, find out where the responses of the current LLM are lacking. This could be either due to several reasons, such as: 1- The dataset used for training does not contain the desired information. 2- Provided response does not contain the desired granularity. 3- Provided response is biased. \n",
    "- __Define and clean the dataset__: Based on the requirements and outcome of previous step, select a dataset of choice that we want for augmentation.\n",
    "- __Clean the dataset__: Self explanatory, but to expand on it a bit on it, make sure that the dataset is in a format that aligns with the queries we will be asking. For example, for historic events, we want to have the date of each event at the beginning of the sentence that describes it (this example).\n",
    "- __Vectorize the dataset__: Use an embedding model to vectorize the cleaned queries. This is done once and in most cases we can save the resulted database. \n",
    "\n",
    "After these steps, we have a vectorize dataset that we can use for RAG. When user ask a question, we project its query into the embedding space (using the same embedding we used for creating the dataset). We then select a number of entries from the database that are closest to the embedding of the query, using a metric (usually cosine similarity). Based on the window size of the LLM, we then augment the original query with as many entries from the database as possible. The resulted augmented query is then sent to the LLM and the response is received. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we are using `gpt-3.5-turbo-instruct` model. This model training data is up to middle of 2021 and does not contain information on year 2023. To illustrate this, we ask the following two questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"your-key\"\n",
    ")\n",
    "\n",
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The last king of England to be coronated was King George VI on May 12, 1937.\n"
     ]
    }
   ],
   "source": [
    "coronation_prompt = \"\"\"\n",
    "Question: \"When was the last king of Englad coronated?\"\n",
    "Answer:\n",
    "\"\"\"\n",
    "initial_coronation_answer = client.completions.create(\n",
    "    model=COMPLETION_MODEL_NAME, prompt=coronation_prompt, max_tokens=150\n",
    ")\n",
    "print(initial_coronation_answer.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The last version of GPT (Generative Pre-trained Transformer) is GPT-3 (Generative Pre-trained Transformer 3) which was released in June 2020.\n"
     ]
    }
   ],
   "source": [
    "gpt_v_prompt = \"\"\"\n",
    "Question: \"What is the last version of GPT?\"\n",
    "Answer:\n",
    "\"\"\"\n",
    "initial_gpt_v_answer = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=gpt_v_prompt, max_tokens=150)\n",
    "print(initial_gpt_v_answer.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been a coronation of king of Englad in 2023 and the latest iteration of GPT is GPT-4, which was released in April. To augment the queries, we use [2023](https://en.wikipedia.org/wiki/2023) which contains the overview of events and developments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of dataset and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\n",
    "    \"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&exlimit=1&titles=2023&explaintext=1&formatversion=2&format=json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step to clean the data, which is in html format, and later store it in a dataframe, we remove all the empty lines and section lines that contain `==` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"text\"] = resp.json()[\"query\"][\"pages\"][0][\"extract\"].split(\"\\n\")\n",
    "df = df[(df[\"text\"].str.len() > 0) & (~df[\"text\"].str.startswith(\"==\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two celss further clean the prepare the data. It ensure that each entry in the dataset has its own date tag, followed by a hyphen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "\n",
    "def is_date(string: str) -> Tuple[bool, Union[None, str]]:\n",
    "    possible_date_string = string.split(\"–\")[0].strip()\n",
    "    try:\n",
    "        _ = parse(possible_date_string)\n",
    "        return True, possible_date_string\n",
    "    except:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = None\n",
    "for _, row in df.iterrows():\n",
    "    is_a_date, date = is_date(row[\"text\"])\n",
    "    if is_a_date:\n",
    "        current_date = date\n",
    "    if current_date is not None and (not is_a_date):\n",
    "        row[\"text\"] = current_date + \" – \" + row[\"text\"]\n",
    "\n",
    "df = df[df[\"text\"].str.contains(\"–\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to feed the cleaned dataset into an embedding model. After each entry got its embedding, we add those to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "batch_size = 100\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, len(df), batch_size):\n",
    "    response = client.embeddings.create(\n",
    "        input=df.iloc[idx : idx + batch_size][\"text\"].tolist(), model=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    embeddings.extend([data.embedding for data in response.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embeddings\"] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the resulted dataset (text plus embeddings) for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query and Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to make some queries with the created embeddings. We first read the saved dataset and convert the embeddings to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"embeddings.csv\")\n",
    "df[\"embeddings\"] = df[\"embeddings\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells are necessary to get the embedding for a given query and measuring distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def get_embedding(text: str, model_name: str) -> List[float]:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model_name).data[0].embedding\n",
    "\n",
    "\n",
    "def distances_from_embeddings(\n",
    "    query_embedding: List[float], embeddings: List[List[float]], distance_metric=\"cosine\"\n",
    ") -> List[float]:\n",
    "    distance_metrics = {\n",
    "        \"cosine\": distance.cosine,\n",
    "        \"L1\": distance.cityblock,\n",
    "        \"L2\": distance.euclidean,\n",
    "        \"inf\": distance.chebyshev,\n",
    "    }\n",
    "    distances = [distance_metrics[distance_metric](query_embedding, embedding) for embedding in embeddings]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_rows_by_relevance` is especially important as it creates an embedding for the question and then sorts the dataframe using the cosine similarity distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_by_relevance(question: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    q_embedding = get_embedding(question, model_name=EMBEDDING_MODEL_NAME)\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"distances\"] = distances_from_embeddings(\n",
    "        query_embedding=q_embedding, embeddings=df_copy[\"embeddings\"].values, distance_metric=\"cosine\"\n",
    "    )\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function, `create_prompt`, we can finally put everything together. We build our custom query using the question and closest entries to the query embedding, while ensuring that we do not go beyond the window threshold of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def create_prompt(question: str, df: pd.DataFrame, max_token_count: int) -> str:\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question\n",
    "can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + len(tokenizer.encode(question))\n",
    "    context = []\n",
    "\n",
    "    for text in get_rows_by_relevance(question, df)[\"text\"].values:\n",
    "        current_token_count += len(tokenizer.encode(text))\n",
    "        if current_token_count < max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we wrap the API point for receiving the response from the LLM in the following function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, df: pd.DataFrame, max_prompt_tokens: int = 1500, max_answer_tokens: int = 500)->Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "\n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = create_prompt(question, df, max_prompt_tokens)\n",
    "\n",
    "    try:\n",
    "        response = client.completions.create(model=COMPLETION_MODEL_NAME, prompt=prompt, max_tokens=max_answer_tokens)\n",
    "        return response.choices[0].text, prompt\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\", prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the newly implemented RAG mechanism, we can test again the response of the LLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The last coronation of a king in England was on May 6, 2023, when Charles III and Camilla were crowned as the King and Queen of the United Kingdom and the other Commonwealth realms. However, the last king to be coronated solely as the King of England was George V in 1911.\n"
     ]
    }
   ],
   "source": [
    "resp, prompt = answer_question(\"When was the last king of England coronated?\", df)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPT-4\n"
     ]
    }
   ],
   "source": [
    "resp, prompt = answer_question(\"What is the latest version GPT?\", df)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both responses are improve and correctly point the to the date of last coronation and latest iteration of GPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
